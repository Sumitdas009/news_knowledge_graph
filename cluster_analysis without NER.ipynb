{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "from newspaper import Article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = pd.read_csv('sample_data.csv')\n",
    "article.columns = ['idx','tags','text','genre','cluster']\n",
    "article = article.head(100)\n",
    "article = article.dropna()\n",
    "article.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import three lists: titles, links and wikipedia synopses\n",
    "titles = article.tags.tolist()\n",
    "\n",
    "synopses = article.text.tolist()\n",
    "    \n",
    "genres = article.genre.tolist()\n",
    "# print (genres)\n",
    "print(str(len(titles)) + ' titles')\n",
    "print(str(len(synopses)) + ' synopses')\n",
    "print(str(len(genres)) + ' genres')\n",
    "# synopses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates index for each item in the corpora (in this case it's just rank) and I'll use this for scoring later\n",
    "ranks = []\n",
    "\n",
    "for i in range(0,len(titles)):\n",
    "    ranks.append(i)\n",
    "\n",
    "# ranks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in synopses:\n",
    "    allwords_stemmed = tokenize_and_stem(i)\n",
    "    totalvocab_stemmed.extend(allwords_stemmed)\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n",
    "\n",
    "# totalvocab_stemmed[0]\n",
    "# totalvocab_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "# vocab_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=20000,\n",
    "                                 min_df=0.1, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(synopses)\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "# terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(titles))\n",
    "print(len(ranks))\n",
    "print(len(synopses))\n",
    "print(len(clusters))\n",
    "print(len(genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "articles = { 'title': titles, 'rank': ranks, 'synopsis': synopses, 'cluster': clusters, 'genre': genres }\n",
    "#print(articles)\n",
    "frame = pd.DataFrame(articles, index = [clusters], columns = ['rank', 'title', 'cluster', 'genre'])\n",
    "frame.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = frame['rank'].groupby(frame['cluster'])\n",
    "\n",
    "grouped.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    for ind in order_centroids[i, :6]:\n",
    "        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print()\n",
    "    print()\n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    for title in frame.ix[i]['title'].values.tolist():\n",
    "        print(' %s,' % title, end='')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidimensional scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # for os.path.basename\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "MDS()\n",
    "\n",
    "# two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip any proper nouns (NNP) or plural proper nouns (NNPS) from a text\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def strip_proppers_POS(text):\n",
    "    tagged = pos_tag(text.split()) #use NLTK's part of speech tagger\n",
    "    non_propernouns = [word for word,pos in tagged if pos != 'NNP' and pos != 'NNPS']\n",
    "    return non_propernouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing document clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up colors per clusters using a dict\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n",
    "\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {0: 'Trump, Hillary', \n",
    "                 1: 'Police, killed, murders', \n",
    "                 2: 'Father, New York, brothers', \n",
    "                 3: 'Dance, singing, love', \n",
    "                 4: 'Killed, soldiers, captain'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "\n",
    "# set up plot\n",
    "# fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
    "# ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "# #iterate through groups to layer the plot\n",
    "# #note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "# for name, group in groups:\n",
    "#     ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=cluster_names[name], color=cluster_colors[name], mec='none')\n",
    "#     ax.set_aspect('auto')\n",
    "#     ax.tick_params(\\\n",
    "#         axis= 'x',          # changes apply to the x-axis\n",
    "#         which='both',      # both major and minor ticks are affected\n",
    "#         bottom='off',      # ticks along the bottom edge are off\n",
    "#         top='off',         # ticks along the top edge are off\n",
    "#         labelbottom='off')\n",
    "#     ax.tick_params(\\\n",
    "#         axis= 'y',         # changes apply to the y-axis\n",
    "#         which='both',      # both major and minor ticks are affected\n",
    "#         left='off',      # ticks along the bottom edge are off\n",
    "#         top='off',         # ticks along the top edge are off\n",
    "#         labelleft='off')\n",
    "    \n",
    "# ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "# #add label in x,y position with the label as the film title\n",
    "# for i in range(len(df)):\n",
    "#     ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=8)  \n",
    "\n",
    "    \n",
    "    \n",
    "# plt.show() #show the plot\n",
    "\n",
    "#uncomment the below to save the plot if need be\n",
    "#plt.savefig('clusters_small_noaxes.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define custom toolbar location\n",
    "class TopToolbar(mpld3.plugins.PluginBase):\n",
    "    \"\"\"Plugin for moving toolbar to top of figure\"\"\"\n",
    "\n",
    "    JAVASCRIPT = \"\"\"\n",
    "    mpld3.register_plugin(\"toptoolbar\", TopToolbar);\n",
    "    TopToolbar.prototype = Object.create(mpld3.Plugin.prototype);\n",
    "    TopToolbar.prototype.constructor = TopToolbar;\n",
    "    function TopToolbar(fig, props){\n",
    "        mpld3.Plugin.call(this, fig, props);\n",
    "    };\n",
    "\n",
    "    TopToolbar.prototype.draw = function(){\n",
    "      // the toolbar svg doesn't exist\n",
    "      // yet, so first draw it\n",
    "      this.fig.toolbar.draw();\n",
    "\n",
    "      // then change the y position to be\n",
    "      // at the top of the figure\n",
    "      this.fig.toolbar.toolbar.attr(\"x\", 150);\n",
    "      this.fig.toolbar.toolbar.attr(\"y\", 400);\n",
    "\n",
    "      // then remove the draw function,\n",
    "      // so that it is not called again\n",
    "      this.fig.toolbar.draw = function() {}\n",
    "    }\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.dict_ = {\"type\": \"toptoolbar\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "#define custom css to format the font and to remove the axis labeling\n",
    "css = \"\"\"\n",
    "text.mpld3-text, div.mpld3-tooltip {\n",
    "  font-family:Arial, Helvetica, sans-serif;\n",
    "}\n",
    "\n",
    "g.mpld3-xaxis, g.mpld3-yaxis {\n",
    "display: none; }\n",
    "\"\"\"\n",
    "\n",
    "# Plot \n",
    "fig, ax = plt.subplots(figsize=(14,6)) #set plot size\n",
    "ax.margins(0.03) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    points = ax.plot(group.x, group.y, marker='o', linestyle='', ms=18, label=cluster_names[name], mec='none', color=cluster_colors[name])\n",
    "    ax.set_aspect('auto')\n",
    "    labels = [i for i in group.title]\n",
    "    \n",
    "    #set tooltip using points, labels and the already defined 'css'\n",
    "    tooltip = mpld3.plugins.PointHTMLTooltip(points[0], labels,\n",
    "                                       voffset=10, hoffset=10, css=css)\n",
    "    #connect tooltip to fig\n",
    "    mpld3.plugins.connect(fig, tooltip, TopToolbar())    \n",
    "    \n",
    "    #set tick marks as blank\n",
    "    ax.axes.get_xaxis().set_ticks([])\n",
    "    ax.axes.get_yaxis().set_ticks([])\n",
    "    \n",
    "    #set axis as blank\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    \n",
    "ax.legend(numpoints=1) #show legend with only one dot\n",
    "\n",
    "mpld3.display() #show the plot\n",
    "\n",
    "#uncomment the below to export to html\n",
    "#html = mpld3.fig_to_html(fig)\n",
    "#print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
